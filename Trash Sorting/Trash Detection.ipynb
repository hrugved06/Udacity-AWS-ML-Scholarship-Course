{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\r\n",
    "import urllib.request\r\n",
    "import boto3, botocore\r\n",
    "\r\n",
    "\r\n",
    "import sagemaker\r\n",
    "from sagemaker import get_execution_role\r\n",
    "\r\n",
    "import mxnet as mx\r\n",
    "mxnet_path = mx.__file__[ : mx.__file__.rfind('/')]\r\n",
    "print(mxnet_path)\r\n",
    "\r\n",
    "role = get_execution_role()\r\n",
    "print(role)\r\n",
    "\r\n",
    "sess = sagemaker.Session()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BUCKET = 'trashdetection'\r\n",
    "PREFIX = 'deeplens-trash'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\r\n",
    "training_image = get_image_uri(sess.boto_region_name, 'image-classification', repo_version=\"latest\")\r\n",
    "print (training_image)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "test_data = 'TestData'\r\n",
    "s3 = boto3.resource('s3')\r\n",
    "object = s3.Object(BUCKET, PREFIX+\"/test.txt\")\r\n",
    "try:\r\n",
    "    object.put(Body=test_data)\r\n",
    "except botocore.exceptions.ClientError as e:\r\n",
    "    if e.response['Error']['Code'] == \"AccessDenied\":\r\n",
    "        #cannot write on the bucket\r\n",
    "        print(\"Bucket \"+BUCKET+\"is not writeable, make sure you have the right permissions\")\r\n",
    "    else:\r\n",
    "        if e.response['Error']['Code'] == \"NoSuchBucket\":\r\n",
    "            #Bucket does not exist\r\n",
    "            print(\"Bucket\"+BUCKET+\" does not exist\")\r\n",
    "        else:\r\n",
    "            raise\r\n",
    "else:\r\n",
    "    print(\"Bucket access is Ok\")\r\n",
    "    object.delete(BUCKET, PREFIX + \"/test.txt\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Prepare data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "It is assumed that your custom dataset's images are present in an S3 bucket and that different classes are separated by named folders, as shown in the following directory structure:\n",
    "```\n",
    "|-deeplens-bucket\n",
    "   |-deeplens-trash\n",
    "\n",
    "    |-images\n",
    "    \n",
    "        |-Compost \n",
    "    \n",
    "        |-Landfill\n",
    "    \n",
    "        |-Recycle\n",
    " ```"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since we are providing the data for you in this example, first we'll download the example data, unzip it and upload it to your bucket."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget https://deeplens-public.s3.amazonaws.com/samples/deeplens-trash/trash-images.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!rm -rf data/ && mkdir -p data\r\n",
    "!mkdir -p data/images\r\n",
    "!unzip -qq trash-images.zip -d data/images\r\n",
    "!rm trash-images.zip"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "%matplotlib inline\r\n",
    "\r\n",
    "def show_images(item_name, images_to_show=-1):\r\n",
    "    _im_list = !ls $IMAGES_DIR/$item_name\r\n",
    "\r\n",
    "    NUM_COLS = 3\r\n",
    "    if images_to_show == -1:\r\n",
    "        IM_COUNT = len(_im_list)\r\n",
    "    else:\r\n",
    "        IM_COUNT = images_to_show\r\n",
    "    \r\n",
    "    print('Displaying images category ' + item_name + ' count: ' + str(IM_COUNT) + ' images.')\r\n",
    "    \r\n",
    "    NUM_ROWS = int(IM_COUNT / NUM_COLS)\r\n",
    "    if ((IM_COUNT % NUM_COLS) > 0):\r\n",
    "        NUM_ROWS += 1\r\n",
    "\r\n",
    "    fig, axarr = plt.subplots(NUM_ROWS, NUM_COLS)\r\n",
    "    fig.set_size_inches(10.0, 10.0, forward=True)\r\n",
    "\r\n",
    "    curr_row = 0\r\n",
    "    for curr_img in range(IM_COUNT):\r\n",
    "        # fetch the url as a file type object, then read the image\r\n",
    "        f = IMAGES_DIR + item_name + '/' + _im_list[curr_img]\r\n",
    "        a = plt.imread(f)\r\n",
    "\r\n",
    "        # find the column by taking the current index modulo 3\r\n",
    "        col = curr_img % NUM_ROWS\r\n",
    "        # plot on relevant subplot\r\n",
    "        if NUM_ROWS == 1:\r\n",
    "            axarr[curr_row].imshow(a)\r\n",
    "        else:\r\n",
    "            axarr[col, curr_row].imshow(a)\r\n",
    "        if col == (NUM_ROWS - 1):\r\n",
    "            # we have finished the current row, so increment row counter\r\n",
    "            curr_row += 1\r\n",
    "\r\n",
    "    fig.tight_layout()       \r\n",
    "    plt.show()\r\n",
    "        \r\n",
    "    # Clean up\r\n",
    "    plt.clf()\r\n",
    "    plt.cla()\r\n",
    "    plt.close()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "IMAGES_DIR = 'data/images/'\r\n",
    "show_images(\"Compost\", images_to_show=3)\r\n",
    "show_images(\"Landfill\", images_to_show=3)\r\n",
    "show_images(\"Recycling\", images_to_show=3)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "DEST_BUCKET = 's3://'+BUCKET+'/'+PREFIX+'/images/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 cp --recursive data/images $DEST_BUCKET --quiet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ensure that the newly created directories containing the downloaded data are structured as shown at the beginning of this tutorial."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 ls $DEST_BUCKET"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "The image and lst files will be converted to RecordIO file internally by the image classification algorithm. But if you want do the conversion, the following cell shows how to do it using the [im2rec](https://github.com/apache/incubator-mxnet/blob/master/tools/im2rec.py) tool. Note that this is just an example of creating RecordIO files. We are **_not_** using them for training in this notebook."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!python $mxnet_path/tools/im2rec.py --list --recursive --test-ratio=0.02 --train-ratio 0.7 trash data/images"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save lst files to S3\n",
    "Training models is easy with Amazon SageMaker. When youâ€™re ready to train in SageMaker, simply specify the location of your data in Amazon S3, and indicate the type and quantity of SageMaker ML instances you need. SageMaker sets up a distributed compute cluster, performs the training, outputs the result to Amazon S3, and tears down the cluster when complete. \n",
    "To use Amazon Sagemaker training we must first transfer our input data to Amazon S3."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3train_lst = 's3://{}/{}/train_lst/'.format(BUCKET, PREFIX)\r\n",
    "s3validation_lst = 's3://{}/{}/validation_lst/'.format(BUCKET, PREFIX)\r\n",
    "\r\n",
    "# upload the lst files to train_lst and validation_lst channels\r\n",
    "!aws s3 cp trash_train.lst $s3train_lst --quiet\r\n",
    "!aws s3 cp trash_val.lst $s3validation_lst --quiet"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's see the size of train, validation and test datasets"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = open('trash_train.lst', 'r')\r\n",
    "train_samples = sum(1 for line in f)\r\n",
    "f.close()\r\n",
    "f = open('trash_val.lst', 'r')\r\n",
    "val_samples = sum(1 for line in f)\r\n",
    "f.close()\r\n",
    "f = open('trash_test.lst', 'r')\r\n",
    "test_samples = sum(1 for line in f)\r\n",
    "f.close()\r\n",
    "print('train_samples:', train_samples)\r\n",
    "print('val_samples:', val_samples)\r\n",
    "print('test_samples:', test_samples)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "This marks the end of the data preparation phase."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train the model\n",
    "\n",
    "Training a good model from scratch can take a long time. Fortunately, we're able to use transfer learning to fine-tune a model that has been trained on millions of images. Transfer learning allows us to train a model to recognize new classes in minutes instead of hours or days that it would normally take to train the model from scratch. Transfer learning requires a lot less data to train a model than from scratch (hundreds instead of tens of thousands)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3_output_location = 's3://{}/{}/output'.format(BUCKET, PREFIX)\r\n",
    "ic = sagemaker.estimator.Estimator(training_image,\r\n",
    "                                         role, \r\n",
    "                                         train_instance_count=1, \r\n",
    "                                         train_instance_type='ml.p2.xlarge',\r\n",
    "                                         train_volume_size = 50,\r\n",
    "                                         train_max_run = 360000,\r\n",
    "                                         input_mode= 'File',\r\n",
    "                                         output_path=s3_output_location,\r\n",
    "                                         sagemaker_session=sess,\r\n",
    "                                         base_job_name='ic-trash')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Apart from the above set of parameters, there are hyperparameters that are specific to the algorithm. These are:\n",
    "\n",
    "* **num_layers**: The number of layers (depth) for the network. We use 18 in this samples but other values such as 50, 152 can be used.\n",
    "* **use_pretrained_model**: Set to 1 to use pretrained model for transfer learning.\n",
    "* **image_shape**: The input image dimensions,'num_channels, height, width', for the network. It should be no larger than the actual image size. The number of channels should be same as the actual image.\n",
    "* **num_classes**: This is the number of output classes for the new dataset. For us, we have \n",
    "* **num_training_samples**: This is the total number of training samples. It is set to 15240 for caltech dataset with the current split.\n",
    "* **mini_batch_size**: The number of training samples used for each mini batch. In distributed training, the number of training samples used per batch will be N * mini_batch_size where N is the number of hosts on which training is run.\n",
    "* **epochs**: Number of training epochs.\n",
    "* **learning_rate**: Learning rate for training.\n",
    "* **top_k**: Report the top-k accuracy during training.\n",
    "* **resize**: Resize the image before using it for training. The images are resized so that the shortest side is of this parameter. If the parameter is not set, then the training data is used as such without resizing.\n",
    "* **precision_dtype**: Training datatype precision (default: float32). If set to 'float16', the training will be done in mixed_precision mode and will be faster than float32 mode\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ic.set_hyperparameters(num_layers=18,\r\n",
    "                             use_pretrained_model=1,\r\n",
    "                             image_shape = \"3,224,224\",\r\n",
    "                             num_classes=3,\r\n",
    "                             mini_batch_size=128,\r\n",
    "                             epochs=10,\r\n",
    "                             learning_rate=0.01,\r\n",
    "                             top_k=2,\r\n",
    "                             num_training_samples=train_samples,\r\n",
    "                             resize = 224,\r\n",
    "                             precision_dtype='float32')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Input data specification\n",
    "Set the data type and channels used for training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "s3images = 's3://{}/{}/images/'.format(BUCKET, PREFIX)\r\n",
    "\r\n",
    "train_data = sagemaker.session.s3_input(s3images, distribution='FullyReplicated', \r\n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\r\n",
    "validation_data = sagemaker.session.s3_input(s3images, distribution='FullyReplicated', \r\n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\r\n",
    "train_data_lst = sagemaker.session.s3_input(s3train_lst, distribution='FullyReplicated', \r\n",
    "                        content_type='application/x-image', s3_data_type='S3Prefix')\r\n",
    "validation_data_lst = sagemaker.session.s3_input(s3validation_lst, distribution='FullyReplicated', \r\n",
    "                             content_type='application/x-image', s3_data_type='S3Prefix')\r\n",
    "\r\n",
    "data_channels = {'train': train_data, 'validation': validation_data, \r\n",
    "                 'train_lst': train_data_lst, 'validation_lst': validation_data_lst}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Start the training\n",
    "Start training by calling the fit method in the estimator"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ic.fit(inputs=data_channels, logs=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "MODEL_PATH = ic.model_data\r\n",
    "print(MODEL_PATH)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Deploy to a Sagemaker endpoint\n",
    "After training your model is complete, you can test your model by asking it to predict the class of a sample trash image that the model has not seen before. This step is called inference.\n",
    "\n",
    "Amazon SageMaker provides an HTTPS endpoint where your machine learning model is available to provide inferences. For more information see the [Amazon SageMaker documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/how-it-works-hosting.html)."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ic_infer = ic.deploy(initial_instance_count=1, instance_type='local')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test the images against the endpoint\n",
    "We will use the test images that were kept aside for testing."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "object_categories = ['Compost', 'Landfill', 'Recycling']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from IPython.display import Image, display\r\n",
    "import json\r\n",
    "import numpy as np\r\n",
    "\r\n",
    "\r\n",
    "def test_model():\r\n",
    "    preds = []\r\n",
    "    acts  = []\r\n",
    "    num_errors = 0\r\n",
    "    with open('trash_test.lst', 'r') as f:\r\n",
    "        for line in f:\r\n",
    "            stripped_line = str(line.strip()).split(\"\\t\")\r\n",
    "            file_path = stripped_line[2]\r\n",
    "            category = int(float(stripped_line[1]))\r\n",
    "            with open(IMAGES_DIR + stripped_line[2], 'rb') as f:\r\n",
    "                payload = f.read()\r\n",
    "                payload = bytearray(payload)\r\n",
    "\r\n",
    "                ic_infer.content_type = 'application/x-image'\r\n",
    "                result = json.loads(ic_infer.predict(payload))\r\n",
    "            # the result will output the probabilities for all classes\r\n",
    "            # find the class with maximum probability and print the class index\r\n",
    "            index = np.argmax(result)\r\n",
    "            act = object_categories[category]\r\n",
    "            pred = object_categories[index]\r\n",
    "            conf = result[index]\r\n",
    "            print(\"Result: Predicted: {}, Confidence: {:.2f}, Actual: {} \".format(pred, conf, act))\r\n",
    "            acts.append(category)\r\n",
    "            preds.append(index)\r\n",
    "            if (pred != act):\r\n",
    "                num_errors += 1\r\n",
    "                print('ERROR on image -- Predicted: {}, Confidence: {:.2f}, Actual: {}'.format(pred, conf, act))\r\n",
    "            display(Image(filename=IMAGES_DIR + stripped_line[2], width=100, height=100))\r\n",
    "\r\n",
    "    return num_errors, preds, acts\r\n",
    "num_errors, preds, acts = test_model()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.metrics import confusion_matrix\r\n",
    "import numpy as np\r\n",
    "import itertools\r\n",
    "COLOR = 'green'\r\n",
    "plt.rcParams['text.color'] = COLOR\r\n",
    "plt.rcParams['axes.labelcolor'] = COLOR\r\n",
    "plt.rcParams['xtick.color'] = COLOR\r\n",
    "plt.rcParams['ytick.color'] = COLOR\r\n",
    "def plot_confusion_matrix(cm, classes,\r\n",
    "                          class_name_list,\r\n",
    "                          normalize=False,\r\n",
    "                          title='Confusion matrix',\r\n",
    "                          cmap=plt.cm.GnBu):\r\n",
    "    plt.figure(figsize=(7,7))\r\n",
    "    plt.grid(False)\r\n",
    "\r\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\r\n",
    "    plt.title(title)\r\n",
    "    tick_marks = np.arange(len(classes))\r\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\r\n",
    "    plt.yticks(tick_marks, classes)\r\n",
    "    fmt = '.2f' if normalize else 'd'\r\n",
    "    thresh = cm.max() / 2.\r\n",
    "    for i, j in itertools.product(range(cm.shape[0]), \r\n",
    "                                  range(cm.shape[1])):\r\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\r\n",
    "                 horizontalalignment=\"center\",\r\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\r\n",
    "    plt.tight_layout()\r\n",
    "    plt.gca().set_xticklabels(class_name_list)\r\n",
    "    plt.gca().set_yticklabels(class_name_list)\r\n",
    "    plt.ylabel('True label')\r\n",
    "    plt.xlabel('Predicted label')\r\n",
    "\r\n",
    "def create_and_plot_confusion_matrix(actual, predicted):\r\n",
    "    cnf_matrix = confusion_matrix(actual, np.asarray(predicted),labels=range(len(object_categories)))\r\n",
    "    plot_confusion_matrix(cnf_matrix, classes=range(len(object_categories)), class_name_list=object_categories)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Display confusion matrix showing 'true' and 'predicted' labels\n",
    "\n",
    "A confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known. It's a table  with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table). The diagonal values in the table indicate a match between the predicted class and the actual class. \n",
    "\n",
    "For more details go to [Confusion matrix](https://en.wikipedia.org/wiki/Confusion_matrix) (Wikipedia)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "create_and_plot_confusion_matrix(acts, preds)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Approximate costs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "As of 03/11/2020 and based on the pricing information displayed on the page: https://aws.amazon.com/sagemaker/pricing/, here's the costs you can expect in a 24 hour period:\n",
    "\n",
    " - Notebook instance cost **\\\\$6** Assuming you choose ml.t3.xlarge (\\\\$0.233/hour) instance. This can vary based on the size of instance you choose.\n",
    " - Training costs **\\\\$1.05** : Assuming you will run about 10 training runs in a 24 hour period using the sample dataset provided. The notebook uses a p2.xlarge (\\\\$1.26/hour) instance\n",
    " - Model hosting **\\\\$6.72** : Assuming you use the ml.m4.xlarge (\\\\$0.28/hour) instance running for 24 hours. \n",
    " \n",
    "*NOTE* : To save on costs, stop your notebook instances and delete the model edpoint when not in use"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "sess.delete_endpoint(ic_infer.endpoint)\r\n",
    "print(\"Completed\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import glob"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!rm -rf data/$PREFIX/tmp && mkdir -p data/$PREFIX/tmp\r\n",
    "!aws s3 cp $MODEL_PATH data/$PREFIX/tmp\r\n",
    "!tar -xzvf data/$PREFIX/tmp/model.tar.gz -C data/$PREFIX/tmp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "params_file_name = glob.glob('./data/' + PREFIX + '/tmp/*.params')[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!mv $params_file_name data/$PREFIX/tmp/image-classification-0000.params"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!tar -cvzf ./model.tar.gz -C data/$PREFIX/tmp ./image-classification-0000.params ./image-classification-symbol.json"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!aws s3 cp model.tar.gz $MODEL_PATH"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_mxnet_p36",
   "language": "python",
   "name": "conda_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}